---
title: "Melodic Trajectories"
author: "Gijs Oliemans"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
    self_contained: false
---



```{r setup, include=FALSE}

library(flexdashboard)
library(dplyr)
library(tidyverse)
library(lubridate)
library(spotifyr)
library(plotly)
library(shiny)
library(treemap)
library(compmus)
library(cowplot)
library(patchwork)
library(gridExtra)
```



```{r combining, include=FALSE}
corpus <- readRDS(file="data/corpus-data.RDS")

```
```{r basic transformations }
# basic data transformations
corpus <- corpus %>%
  mutate(track_info = paste(track.name, "-", track.album.name)) %>%
  mutate(year = substr(track.album.release_date, 1, 4))

```

```{r album_corpus , message=FALSE, include=FALSE}
# corpus grouped by albums

# Grouping and summarizing the dataset
album_corpus <- corpus %>%
  group_by(artist, track.album.name, track.album.release_date) %>%
  summarise(
    danceability_min = min(danceability, na.rm = TRUE),
    danceability_max = max(danceability, na.rm = TRUE),
    danceability_median = median(danceability, na.rm = TRUE),
    danceability_mean = mean(danceability, na.rm = TRUE),
    danceability_sd = sd(danceability, na.rm = TRUE), # Standard deviation for danceability
    
    duration_min = min(track.duration_ms, na.rm = TRUE),
    duration_max = max(track.duration_ms, na.rm = TRUE),
    duration_median = median(track.duration_ms, na.rm = TRUE),
    duration_mean = mean(track.duration_ms, na.rm = TRUE),
    duration_sd = sd(track.duration_ms, na.rm = TRUE), # Standard deviation for danceability
    
    energy_min = min(energy, na.rm = TRUE),
    energy_max = max(energy, na.rm = TRUE),
    energy_median = median(energy, na.rm = TRUE),
    energy_mean = mean(energy, na.rm = TRUE),
    energy_sd = sd(energy, na.rm = TRUE), # Standard deviation for energy
    
    loudness_min = min(loudness, na.rm = TRUE),
    loudness_max = max(loudness, na.rm = TRUE),
    loudness_median = median(loudness, na.rm = TRUE),
    loudness_mean = mean(loudness, na.rm = TRUE),
    loudness_sd = sd(loudness, na.rm = TRUE), # Standard deviation for loudness
    
    speechiness_min = min(speechiness, na.rm = TRUE),
    speechiness_max = max(speechiness, na.rm = TRUE),
    speechiness_median = median(speechiness, na.rm = TRUE),
    speechiness_mean = mean(speechiness, na.rm = TRUE),
    speechiness_sd = sd(speechiness, na.rm = TRUE), # Standard deviation for speechiness
    
    acousticness_min = min(acousticness, na.rm = TRUE),
    acousticness_max = max(acousticness, na.rm = TRUE),
    acousticness_median = median(acousticness, na.rm = TRUE),
    acousticness_mean = mean(acousticness, na.rm = TRUE),
    acousticness_sd = sd(acousticness, na.rm = TRUE), # Standard deviation for acousticness
    
    liveness_min = min(liveness, na.rm = TRUE),
    liveness_max = max(liveness, na.rm = TRUE),
    liveness_median = median(liveness, na.rm = TRUE),
    liveness_mean = mean(liveness, na.rm = TRUE),
    liveness_sd = sd(liveness, na.rm = TRUE), # Standard deviation for liveness
    
    valence_min = min(valence, na.rm = TRUE),
    valence_max = max(valence, na.rm = TRUE),
    valence_median = median(valence, na.rm = TRUE),
    valence_mean = mean(valence, na.rm = TRUE),
    valence_sd = sd(valence, na.rm = TRUE), # Standard deviation for valence
    
    tempo_min = min(tempo, na.rm = TRUE),
    tempo_max = max(tempo, na.rm = TRUE),
    tempo_median = median(tempo, na.rm = TRUE),
    tempo_mean = mean(tempo, na.rm = TRUE),
    tempo_sd = sd(tempo, na.rm = TRUE) # Standard deviation for tempo
  ) %>%
  ungroup() # Ensure the summarisation doesn't carry over into subsequent operations


```

```{r corpus_by_artist, include=FALSE} 
# corpus grouped by artists (for treemap)
corpus_by_artist <- corpus %>%
  mutate(track_duration = track.duration_ms / 60000) %>%
  group_by(artist) %>%
  summarize(
    track_count = n(),  # Count of tracks per artist
    average_duration = mean(track_duration),
    track_duration = track_duration# Optional: average duration of tracks per artist
  )
print(corpus_by_artist)
```

```{r outliers, include=FALSE}
corpus_arranged <- corpus %>%
  filter(track.duration_ms >= 12000) %>%
  mutate(combination = energy + valence) %>%
  arrange(combination) %>%
  mutate(label_position = case_when(
    row_number() == 1 ~ "Lowest",
    row_number() == n() ~ "Highest",
    TRUE ~ NA_character_
  ))

```
# How do two outliers of my corpus look like? (w9){.storyboard data-navmenu=Homework} 

### How doe the chroma features look like?
```{r data_setup, include=FALSE}
# Extracting track.id for Lowest combination
lowest_track_id <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  pull(track.id)

# Extracting track.id for Highest combination
highest_track_id <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  pull(track.id)

print(lowest_track_id)
print(highest_track_id)
# Extracting track info for Lowest combination
lowest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)

# Extracting track info for Highest combination
highest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)
```


```{r chroma_lowest, echo=FALSE, fig.width=13, fig.height=8}
lowest_energy_valence <-
  get_tidy_audio_analysis(lowest_track_id) |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

highest_energy_valence <-
  get_tidy_audio_analysis(highest_track_id) |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

##### Chromagrams
lowest_energy_valence_chromagram <-
  lowest_energy_valence |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title= lowest_track_name_album,x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

highest_energy_valence_chromagram <-
  highest_energy_valence |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title= highest_track_name_album,x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()




plot_grid(lowest_energy_valence_chromagram, highest_energy_valence_chromagram, 
          ncol=1
)
```

***
In the two graphs on the left side, two tracks are displayed. These tracks are outliers in my corpus, selected based on a combination of two factors: the Valence & Energy scores assigned by Spotify. I've chosen these types of outliers to explore the relationship between pitch classes and their occurrences with the Valence & Energy scores from Spotify.

The first chromagram displays a song with the lowest combination of valence & energy. In this song by Miles Davis, we see an overall absence of pitch classes, with a strong occurrence of a few (3-4) pitch classes.

In comparison with the lower chromagram, which displays a song with the highest combination of valence & energy, we see a higher occurrence of pitch classes (9-11) throughout the whole song. This energetic song by Youssou N'Dour is typical for the Mbalax genre, known for its fast-stroking percussion, as also present in this song. The percussion would explain the diversity in occurrences of pitch classes.

Overall, we could argue that the presence of a high diversity in pitch classes triggers Spotify to assign a higher Valence & Energy score.

### What does the structure of these outliers look like in self similarity matrices?
```{r data_setting_up}
# Extracting track.id for Lowest combination
lowest_track_id <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  pull(track.id)

# Extracting track.id for Highest combination
highest_track_id <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  pull(track.id)

# Extracting track info for Lowest combination
lowest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)

# Extracting track info for Highest combination
highest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)

lowest_track_artist <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  pull(artist)

highest_track_artist <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  pull(artist)
```

```{r selfsimilarity, include=FALSE}

## getting audio analysis
lowest_track <- readRDS(file = "data/lowest_track_tidy_analysis-data.RDS")
highest_track <- readRDS(file = "data/highest_track_tidy_analysis-data.RDS")
# LOWEST CHROMA
plot_lowest_chroma <- lowest_track |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "Chroma")

#LOWEST TIMBRE
plot_lowest_timbre <- lowest_track |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "Timbre")
  
# HIGHEST CHROMA
plot_highest_chroma <- highest_track |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "")


plot_highest_timbre <- highest_track |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "")

```

```{r plotting_self_similarity, echo=FALSE}

combined_plot <- (plot_lowest_chroma | plot_lowest_timbre) /
                 (plot_highest_chroma | plot_highest_timbre)


# print(combined_plot)

plot_grid(
  plot_lowest_chroma, plot_lowest_timbre, plot_highest_chroma, plot_highest_timbre,
  ncol=2,
  labels=c("A", "", "B", ""),
  label_fontface = "plain",
  hjust = 0,
  vjust = 10
  )


```

***
These 4 graphs display the pitch and timbre self-similarity matrix of the same two songs as discussed in the "Chroma features" slide. Row "A" displays the matrices for the song by Miles Davis. Row "B" displays the matrices for the song by Youssou N'Dour.

*Row A*

For the song with the lowest combination of energy & valence, we see a clear overview of the structure of the song in both the chroma- and timbre-based self-similarity matrices. For both matrices, one could count 4-5 different parts throughout the song.

*Row B*

For the song with the highest combination of energy & valence, we see a notable difference in the chroma- and timbre-based self-similarity matrices. For the chroma-based matrix, there is no recognizable structure throughout the song. I think this is due to the fact of the percussion parts in this song, which provide a high diversity in pitch classes (due to their timbre and overtones) and therefore clutter the pitch class data. For the timbre-based matrix, clear structured parts of the song are recognizable.

Overall, we can argue that a high diversity in pitch classes can clutter the self-similarity matrix of a song, while timbre can still clearly display the structure of the same song. For a song with a lower diversity in pitch classes, this is less of a problem, and both chroma- and timbre-based self-similarity matrices display the same structure of a song.







# Keygrams, histograms and stdev (w10) {.storyboard data-navmenu=Homework} 
### Keygrams
```{r chord_template, include=FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r formatting, include=FALSE}
lowest_data <-
  get_tidy_audio_analysis(lowest_track_id) |>
  compmus_align(beats, segments) |>
  select(beats) |>
  unnest(beats) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

highest_data <-
  get_tidy_audio_analysis(highest_track_id) |>
  compmus_align(beats, segments) |>
  select(beats) |>
  unnest(beats) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

plot_lowest <-
  lowest_data |> 
    compmus_match_pitch_template(
      key_templates,         # Change to chord_templates if descired
      method = "euclidean",  # Try different distance metrics
      norm = "manhattan"     # Try different norms
    ) |>
    ggplot(
      aes(x = start + duration / 2, width = duration, y = name, fill = d)
    ) +
    geom_tile() +
    scale_fill_viridis_c(guide = "none") +
    theme_minimal() +
    labs(x = "Time (s)", y = "", title = lowest_track_name_album)

plot_highest <-
  highest_data |> 
    compmus_match_pitch_template(
      key_templates,         # Change to chord_templates if descired
      method = "euclidean",  # Try different distance metrics
      norm = "manhattan"     # Try different norms
    ) |>
    ggplot(
      aes(x = start + duration / 2, width = duration, y = name, fill = d)
    ) +
    geom_tile() +
    scale_fill_viridis_c(guide = "none") +
    theme_minimal() +
    labs(x = "Time (s)", y = "", title = highest_track_name_album)
```
```{r plotting_key_analysis, echo=FALSE}
plot_grid(plot_lowest, plot_highest, 
          ncol=1
)

```

***
On the left we see the keygrams for two songs in my corpus. These tracks are extremes in my corpus, selected based on a combination of two factors: the Valence & Energy scores assigned by Spotify. 

"Julien dans l'ascenseur" by Miles Davis is the least extreme song regarding the Valence & Energy score. Throughout the song it's not super clear to see exactly which keys are used, but there are some moments on which we wee a change. One of these moments is on the  20-25s mark, which would make sense since around this time a Miles Davis starts his trumpet part. The trumpet part seems to trigger the different keys. 

"Dawal - Version Mbalax" by Youssou N'Dour is the most extreme song regarding the Valence & Energy score. For the major part of the song, there is not really one key recognisable. This is most likely due to the amount of percussion instruments in the song. If you squint your eyes, you can kind of see a repeating pattern of dark blue throughout the whole song. 


### Histograms 


```{r}

corpus$mode <- as.factor(corpus$mode)
corpus_counts <- corpus %>%
  group_by(playlist_name, key, mode) %>%
  summarise(count = n(), .groups = 'drop')

# Calculate the total count of tracks for each artist
total_counts <- corpus %>%
  group_by(playlist_name) %>%
  summarise(total = n(), .groups = 'drop')

# Merge the counts with the total and calculate the proportion
corpus_proportions <- merge(corpus_counts, total_counts, by = "playlist_name") %>%
  mutate(proportion = count / total)

# Ensure 'key' is a factor with levels in the correct order for plotting
corpus_proportions$key <- factor(corpus_proportions$key, levels = as.character(0:11))


# Assuming 'key' is stored as numeric and needs to be converted for meaningful axis labels
key_labels_x <- c("C", "C#", "D", "Eb", "E", "F", "F#", "G", "Ab", "A", "Bb", "B")
# Create the plot
# Assuming the rest of your plotting code remains the same
key_mode_plot <- ggplot(corpus_proportions, aes(x=key, y=proportion, fill=mode)) +
  geom_bar(stat="identity", position="stack") +
  scale_x_discrete(labels=key_labels_x) +  # Assuming key_labels_x is defined as before
  scale_fill_manual(values=c("red", "blue"), labels=c("minor", "major")) +
  facet_wrap(~ playlist_name, scales="free_y") +
  labs(x="Key", y="Proportion", title="Proportions of Keys and Modes in Artist Discographies") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, hjust=1))

# Convert to plotly object
interactive_plot <- ggplotly(key_mode_plot)

# Display the interactive plot
interactive_plot

```

***

Displayed on this page are the proportional distribution plots of musical keys for each artist. To account for the varying catalog sizes, the frequency of tracks in specific keys is presented as a percentage of the total number of works in each artist's entire catalog, ensuring a normalized comparison.

For me, the most interesting distribution of keys is the one of the Beatles. There we see high occurrences of the following keys: C, D, E, G, A. (all white keys on a piano) These keys could be classified as more easy to use/understand. 

Upon closer inspection, we see the roughly the same distribution for the following artists: Caetano Veloso, Radiohead and RHCP. These artist and the beatles roughly show the same pattern of often-used keys: C, D, E, F, G, A, and B.

### Standard Deviation

```{r stdev, echo=FALSE}
scattertje <- ggplot(album_corpus, aes(
    x = tempo_mean,
    y = tempo_sd,
    colour = artist,
    size = duration_mean,
    alpha = loudness_mean
  )) + 
  geom_point() +
  geom_rug() +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Artist"
  ) +
  guides(
    size = guide_legend(title = "Duration (min)"),
    alpha = guide_legend(title = "Volume (dBFS)")
  )

ggplotly(scattertje)

```

***

In the scatterplot we see the albums as dots, colored per artist. On the x and y axis display the mean and standard deviation of tempo respectively. Because the standard deviation is calculated per album, it's generally quite high but also for most artists the same. For artist like Caetano Veloso, Fela Kuti and Miles Davis we see their albums spread out over both axes. For other artists we see their albums are more clustered together.

# Introduction {.storyboard data-navmenu=Introduction}
### Introduction  
```{r intro_overview, echo=FALSE}


```

On this website we explore my selected corpus and aim to research the musical journey of artist throughout their careers by leveraging the track audio featurs from the Spotify API.

This corpus encompasses a diverse array of music from nine artists: The Beatles, Red Hot Chili Peppers (RHCP), Fela Kuti, Daft Punk, Björk, Miles Davis, Radiohead, Youssou N'Dour, and Caetano Veloso. It spans various genres, including rock, funk, Afrobeat, electronic, jazz, alternative, Mbalax, and Tropicália, representing significant periods and shifts in the music industry. This selection is carefully chosen to enable a comprehensive analysis of musical evolution and the influence of cultural and socio-political contexts on musical expression.

The structure of the corpus, centered around studio-recorded albums, allows for comparisons of musical styles and thematic content. The Beatles and RHCP offer a lens into the development of rock and its subgenres, while Fela Kuti and Miles Davis add the complexity of Afrobeat and jazz. Daft Punk and Björk extend the exploration to the innovative realms of electronic music, and Radiohead combines rock with electronic sounds. The inclusion of Youssou N'Dour and Caetano Veloso incorporates the unique rhythms and melodies of Senegal and Brazil, respectively, enriching the corpus's cultural diversity.

The albums selected showcase each artist's characteristic style and include tracks that offer a contrast to their typical sound, providing a broad base for analysis. This approach, focusing on studio albums, emphasizes the nuances of production choices and artist intentions. While this corpus is substantial, it remains selective, aiming to reveal trends, contrasts, and the dynamic relationship between music and its cultural backdrop. The objective is to deepen the understanding of how music acts as a reflection of and a force upon the zeitgeist across different regions and times.

### Artist overview

```{r artist_overview, echo=FALSE, fig_width=5,fig.height=4}
# Create a ggplot object
p <- ggplot(corpus_by_artist, aes(x = reorder(artist, track_count), y = track_count)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(x = "Artist Name", y = "Catalog Size", title = "Catalog Size of Artists") +
  coord_flip() # Flipping coordinates for better readability




boxplot <- ggplot(corpus_by_artist, aes(x = artist, y = track_duration)) +
              geom_boxplot() +
              theme_minimal() +
              labs(x = "Artist", y = "Track Length (minutes)", title = "Track Length Distribution Across Artists") +
              theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate artist names for readability

plot_grid(p, boxplot,
          nrow= 2)
```

***
Here comes a sidebar

### Which artist makes the most happy and energetic songs?

```{r explorative_plot, echo=FALSE}
# Step 1: Identify the points
album_corpus <- album_corpus %>%
  mutate(combination = energy_mean + valence_mean) %>%
  arrange(combination) %>%
  mutate(label_position = case_when(
    row_number() == 1 ~ "Lowest",
    row_number() == n() ~ "Highest",
    TRUE ~ NA_character_
  ))

# Prepare the label text
album_corpus$label_text <- ifelse(
  !is.na(album_corpus$label_position),
  paste(album_corpus$artist, album_corpus$track.album.name, sep = ", "),
  NA
)

# Step 2: Plot with annotations
explorative_plot <- ggplot(album_corpus, aes(x = energy_mean, y = valence_mean, color = artist)) +
    geom_point(aes(alpha = 0.4, size = danceability_mean)) +
    geom_text(data = filter(album_corpus, !is.na(label_text)), 
                aes(label = label_text), 
                nudge_x = 0.03, nudge_y = 0.01,  # Adjust label location
                check_overlap = TRUE, 
                size = 2) +  # Adjust label size
    labs(title = "Energy vs. valence colored by Artist with danceability Size",
         x = "Energy",
         y = "Valence",
         color = "Artist",
         size = "Danceability",
         alpha = "Alpha") +
    theme_minimal() +
    guides(size = guide_legend(title = "Valence")) +
    theme(legend.text = element_text(size = 12),  # Increase legend text size
          legend.key.size = unit(1, "lines"))  # Increase legend key size

ggplotly(explorative_plot)

```

***

The plot included in this repository displays the distribution of track descriptives (energy, danceability and valence) of the selected corpus, grouped by albums where artists are displayed with color. All the data is extracted with the Spotify API by combining self constructed playlists. Each playlist contained all the studio album recordings of an artist, live recordings were ignored in this dataset.

Upon a close look of the plot, the albums of each artist seem to cluster both together, where Miles Davis and Björk have the most spread out descriptive variables. Artist like the Beatles, Daft Punk, Fela Kuti and RHCP (Red Hot Chili Peppers) seem to have the most concise album audio features. Youssou N'Dour and Caetano Veloso seem to stay in the middle artists like Miles Davis & Björk and artists with concise album audio features.

### Detailed artist overview

```{r overview, echo=FALSE}
# Static plot code here
# Now, use 'track_info' in the 'text' aesthetic for tooltips
track_plot <- ggplot(corpus, aes(x = energy, y = valence, color = track.album.name, text = track_info, alpha = 0.8)) +
  geom_point() +
  facet_wrap(~artist) +
  labs(title = "Valence vs Energy colored by danceability per artist") + # Title for the color scale
  theme_minimal() +
  guides(alpha = none) + # Optionally, remove the alpha guide if not needed
  theme(legend.text = element_text(size = 12),  # Increase legend text size
        legend.key.size = unit(1, "lines"),  # Increase legend key size
        legend.position = "none")  # Remove the legend

# Convert to plotly object with tooltips
track_plotly <- ggplotly(track_plot, tooltip = "text")

# Print the plotly plot
track_plotly



```

***

Sidebar text

# How much do artists adhere to their albums? {.storyboard data-navmenu=Introduction}
### This will be a page


# Artists {.storyboard data-navmenu="Musical Journey"}
### About page
```{r}

corpus$year

```

### Music
```{r}
1+1
```


# Beatles {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis


# Björk {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# Caetano Veloso {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# Daft Punk {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# Fela Kuti {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# Miles Davis {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# Radiohead {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# RHCP {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

# Youssou N'Dour {.storyboard data-navmenu="Musical Journey"}

### About the artist

### Analysis

