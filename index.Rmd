---
title: "Melodic Trajectories"
author: "Gijs Oliemans"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
---



```{r setup, include=FALSE}

library(flexdashboard)
library(dplyr)
library(tidyverse)
library(lubridate)
library(spotifyr)
library(plotly)
library(shiny)
library(treemap)
library(compmus)
library(cowplot)
library(patchwork)
library(gridExtra)
```

```{r loading, include=FALSE}
# Loading datasets
beatles <- get_playlist_audio_features("", "5YPyQdiRzU0xolryQHqhNv") 
rhcp <- get_playlist_audio_features("", "04rfJgMUHRVg3d7iL3dLPU")
fela_kuti <- get_playlist_audio_features("", "7kdOWMLqjh2nfBPqvZ6zGd")
daft_punk <- get_playlist_audio_features("", "49In2rUl8PCWKC6yAETu9G")
bjork <- get_playlist_audio_features("", "19Ar68GhVwnuSNgw7CtF7y")
miles_davis <- get_playlist_audio_features("", "1qK5njkyEremnIBvPbw36O")
radiohead <- get_playlist_audio_features("", "0GedvaI33dOhxTaJTnh80p")
youssoundour <- get_playlist_audio_features("","23IgCQB3p9ZhJtD4wPwQAC")
caetanoveloso <- get_playlist_audio_features("", "6PEjftDLv1ro1R8y4GwUAn")
```

```{r combining, include=FALSE}
beatles$artist <- "Beatles"
rhcp$artist <- "RHCP"
fela_kuti$artist <- "Fela Kuti"
daft_punk$artist <- "Daft Punk"
bjork$artist <- "Bjork"
miles_davis$artist <- "Miles Davis"
radiohead$artist <- "Radiohead"
youssoundour$artist <- "Youssou N'Dour"
caetanoveloso$artist <- "Caetano Veloso"
corpus <- rbind(beatles, rhcp, fela_kuti, daft_punk, bjork, miles_davis, radiohead, youssoundour, caetanoveloso)

```
```{r basic transformations }
# basic data transformations
corpus <- corpus %>%
  mutate(track_info = paste(track.name, "-", track.album.name))

```

```{r album_corpus , message=FALSE, include=FALSE}
# corpus grouped by albums

# Grouping and summarizing the dataset
album_corpus <- corpus %>%
  group_by(artist, track.album.name, track.album.release_date) %>%
  summarise(
    danceability_min = min(danceability, na.rm = TRUE),
    danceability_max = max(danceability, na.rm = TRUE),
    danceability_median = median(danceability, na.rm = TRUE),
    danceability_mean = mean(danceability, na.rm = TRUE),
    
    energy_min = min(energy, na.rm = TRUE),
    energy_max = max(energy, na.rm = TRUE),
    energy_median = median(energy, na.rm = TRUE),
    energy_mean = mean(energy, na.rm = TRUE),
    
    loudness_min = min(loudness, na.rm = TRUE),
    loudness_max = max(loudness, na.rm = TRUE),
    loudness_median = median(loudness, na.rm = TRUE),
    loudness_mean = mean(loudness, na.rm = TRUE),
    
    speechiness_min = min(speechiness, na.rm = TRUE),
    speechiness_max = max(speechiness, na.rm = TRUE),
    speechiness_median = median(speechiness, na.rm = TRUE),
    speechiness_mean = mean(speechiness, na.rm = TRUE),
    
    acousticness_min = min(acousticness, na.rm = TRUE),
    acousticness_max = max(acousticness, na.rm = TRUE),
    acousticness_median = median(acousticness, na.rm = TRUE),
    acousticness_mean = mean(acousticness, na.rm = TRUE),
    
    liveness_min = min(liveness, na.rm = TRUE),
    liveness_max = max(liveness, na.rm = TRUE),
    liveness_median = median(liveness, na.rm = TRUE),
    liveness_mean = mean(liveness, na.rm = TRUE),
    
    valence_min = min(valence, na.rm = TRUE),
    valence_max = max(valence, na.rm = TRUE),
    valence_median = median(valence, na.rm = TRUE),
    valence_mean = mean(valence, na.rm = TRUE),
    
    tempo_min = min(tempo, na.rm = TRUE),
    tempo_max = max(tempo, na.rm = TRUE),
    tempo_median = median(tempo, na.rm = TRUE),
    tempo_mean = mean(tempo, na.rm = TRUE)
  ) %>%
  ungroup() # Ensure the summarisation doesn't carry over into subsequent operations

# View the summarized data

```

```{r artist_data, include=FALSE} 
# corpus grouped by artists (for treemap)
artist_data <- corpus %>%
  mutate(track_duration = track.duration_ms / 60000) %>%
  group_by(artist) %>%
  summarize(
    track_count = n(),  # Count of tracks per artist
    average_duration = mean(track_duration),
    track_duration = track_duration# Optional: average duration of tracks per artist
  )
print(artist_data)
```

```{r outliers, include=FALSE}
corpus_arranged <- corpus %>%
  filter(track.duration_ms >= 12000) %>%
  mutate(combination = energy + valence) %>%
  arrange(combination) %>%
  mutate(label_position = case_when(
    row_number() == 1 ~ "Lowest",
    row_number() == n() ~ "Highest",
    TRUE ~ NA_character_
  ))

```
### How do the chroma features look like for outliers in my corpus?
```{r data_setup, include=FALSE}
# Extracting track.id for Lowest combination
lowest_track_id <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  pull(track.id)

# Extracting track.id for Highest combination
highest_track_id <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  pull(track.id)

# Extracting track info for Lowest combination
lowest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)

# Extracting track info for Highest combination
highest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)
```


```{r chroma_lowest, echo=FALSE, fig.width=13, fig.height=8}
lowest_energy_valence <-
  get_tidy_audio_analysis(lowest_track_id) |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

highest_energy_valence <-
  get_tidy_audio_analysis(highest_track_id) |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

##### Chromagrams
lowest_energy_valence_chromagram <-
  lowest_energy_valence |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title= lowest_track_name_album,x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

highest_energy_valence_chromagram <-
  highest_energy_valence |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title= highest_track_name_album,x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()




plot_grid(lowest_energy_valence_chromagram, highest_energy_valence_chromagram, 
          ncol=1
)
```

***
In the two graphs on the left side, two tracks are displayed. These tracks are outliers in my corpus, selected based on a combination of two factors: the Valence & Energy scores assigned by Spotify. I've chosen these types of outliers to explore the relationship between pitch classes and their occurrences with the Valence & Energy scores from Spotify.

The first chromagram displays a song with the lowest combination of valence & energy. In this song by Miles Davis, we see an overall absence of pitch classes, with a strong occurrence of a few (3-4) pitch classes.

In comparison with the lower chromagram, which displays a song with the highest combination of valence & energy, we see a higher occurrence of pitch classes (9-11) throughout the whole song. This energetic song by Youssou N'Dour is typical for the Mbalax genre, known for its fast-stroking percussion, as also present in this song. The percussion would explain the diversity in occurrences of pitch classes.

Overall, we could argue that the presence of a high diversity in pitch classes triggers Spotify to assign a higher Valence & Energy score.

### Self Similarity
```{r data_setting_up}
# Extracting track.id for Lowest combination
lowest_track_id <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  pull(track.id)

# Extracting track.id for Highest combination
highest_track_id <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  pull(track.id)

# Extracting track info for Lowest combination
lowest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)

# Extracting track info for Highest combination
highest_track_name_album <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  mutate(combined_info = paste(track_info, artist, sep = " - ")) %>%
  pull(combined_info)

lowest_track_artist <- corpus_arranged %>%
  filter(label_position == "Lowest") %>%
  pull(artist)

highest_track_artist <- corpus_arranged %>%
  filter(label_position == "Highest") %>%
  pull(artist)
```

```{r selfsimilarity, include=FALSE}

## getting audio analysis
lowest_track <-
  get_tidy_audio_analysis(lowest_track_id) |>
  compmus_align(beats, segments) |>
  select(beats) |>
  unnest(beats) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )
highest_track <-
  get_tidy_audio_analysis(highest_track_id) |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  )

# LOWEST CHROMA
plot_lowest_chroma <- lowest_track |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "Chroma")

#LOWEST TIMBRE
plot_lowest_timbre <- lowest_track |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "Timbre")
  
# HIGHEST CHROMA
plot_highest_chroma <- highest_track |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "")


plot_highest_timbre <- highest_track |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre") |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "", title = "")

```

```{r plotting_self_similarity, echo=FALSE}

combined_plot <- (plot_lowest_chroma | plot_lowest_timbre) /
                 (plot_highest_chroma | plot_highest_timbre)


# print(combined_plot)

plot_grid(
  plot_lowest_chroma, plot_lowest_timbre, plot_highest_chroma, plot_highest_timbre,
  ncol=2,
  labels=c("A", "", "B", ""),
  label_fontface = "plain",
  hjust = 0,
  vjust = 10
  )


```

***
These 4 graphs display the pitch and timbre self-similarity matrix of the same two songs as discussed in the "Chroma features" slide. Row "A" displays the matrices for the song by Miles Davis. Row "B" displays the matrices for the song by Youssou N'Dour.

*Row A*

For the song with the lowest combination of energy & valence, we see a clear overview of the structure of the song in both the chroma- and timbre-based self-similarity matrices. For both matrices, one could count 4-5 different parts throughout the song.

*Row B*

For the song with the highest combination of energy & valence, we see a notable difference in the chroma- and timbre-based self-similarity matrices. For the chroma-based matrix, there is no recognizable structure throughout the song. I think this is due to the fact of the percussion parts in this song, which provide a high diversity in pitch classes (due to their timbre and overtones) and therefore clutter the pitch class data. For the timbre-based matrix, clear structured parts of the song are recognizable.

Overall, we can argue that a high diversity in pitch classes can clutter the self-similarity matrix of a song, while timbre can still clearly display the structure of the same song. For a song with a lower diversity in pitch classes, this is less of a problem, and both chroma- and timbre-based self-similarity matrices display the same structure of a song.






### Introduction 
```{r intro_overview, echo=FALSE}


```

On this website we explore my selected corpus and aim to research the musical journey of artist throughout their careers by leveraging the track audio featurs from the Spotify API.

This corpus encompasses a diverse array of music from nine artists: The Beatles, Red Hot Chili Peppers (RHCP), Fela Kuti, Daft Punk, Björk, Miles Davis, Radiohead, Youssou N'Dour, and Caetano Veloso. It spans various genres, including rock, funk, Afrobeat, electronic, jazz, alternative, Mbalax, and Tropicália, representing significant periods and shifts in the music industry. This selection is carefully chosen to enable a comprehensive analysis of musical evolution and the influence of cultural and socio-political contexts on musical expression.

The structure of the corpus, centered around studio-recorded albums, allows for comparisons of musical styles and thematic content. The Beatles and RHCP offer a lens into the development of rock and its subgenres, while Fela Kuti and Miles Davis add the complexity of Afrobeat and jazz. Daft Punk and Björk extend the exploration to the innovative realms of electronic music, and Radiohead combines rock with electronic sounds. The inclusion of Youssou N'Dour and Caetano Veloso incorporates the unique rhythms and melodies of Senegal and Brazil, respectively, enriching the corpus's cultural diversity.

The albums selected showcase each artist's characteristic style and include tracks that offer a contrast to their typical sound, providing a broad base for analysis. This approach, focusing on studio albums, emphasizes the nuances of production choices and artist intentions. While this corpus is substantial, it remains selective, aiming to reveal trends, contrasts, and the dynamic relationship between music and its cultural backdrop. The objective is to deepen the understanding of how music acts as a reflection of and a force upon the zeitgeist across different regions and times.

### Artist overview

```{r treemap_overview, echo=FALSE, fig_width=5,fig.height=4}
# Create a ggplot object
p <- ggplot(artist_data, aes(x = reorder(artist, track_count), y = track_count)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(x = "Artist Name", y = "Catalog Size", title = "Catalog Size of Artists") +
  coord_flip() # Flipping coordinates for better readability




boxplot <- ggplot(artist_data, aes(x = artist, y = track_duration)) +
              geom_boxplot() +
              theme_minimal() +
              labs(x = "Artist", y = "Track Length (minutes)", title = "Track Length Distribution Across Artists") +
              theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate artist names for readability

plot_grid(p, boxplot,
          nrow= 2)
```

### Which artist has the highest valence?

```{r explorative_plot, echo=FALSE}
# Step 1: Identify the points
album_corpus <- album_corpus %>%
  mutate(combination = energy_mean + valence_mean) %>%
  arrange(combination) %>%
  mutate(label_position = case_when(
    row_number() == 1 ~ "Lowest",
    row_number() == n() ~ "Highest",
    TRUE ~ NA_character_
  ))

# Prepare the label text
album_corpus$label_text <- ifelse(
  !is.na(album_corpus$label_position),
  paste(album_corpus$artist, album_corpus$track.album.name, sep = ", "),
  NA
)

# Step 2: Plot with annotations
explorative_plot <- ggplot(album_corpus, aes(x = energy_mean, y = valence_mean, color = artist)) +
    geom_point(aes(alpha = 0.4, size = danceability_mean)) +
    geom_text(data = filter(album_corpus, !is.na(label_text)), 
                aes(label = label_text), 
                nudge_x = 0.03, nudge_y = 0.01,  # Adjust label location
                check_overlap = TRUE, 
                size = 2) +  # Adjust label size
    labs(title = "Energy vs. valence colored by Artist with danceability Size",
         x = "Energy",
         y = "Valence",
         color = "Artist",
         size = "Danceability",
         alpha = "Alpha") +
    theme_minimal() +
    guides(size = guide_legend(title = "Valence")) +
    theme(legend.text = element_text(size = 12),  # Increase legend text size
          legend.key.size = unit(1, "lines"))  # Increase legend key size

ggplotly(explorative_plot)

```

***

The plot included in this repository displays the distribution of track descriptives (energy, danceability and valence) of the selected corpus, grouped by albums where artists are displayed with color. All the data is extracted with the Spotify API by combining self constructed playlists. Each playlist contained all the studio album recordings of an artist, live recordings were ignored in this dataset.

Upon a close look of the plot, the albums of each artist seem to cluster both together, where Miles Davis and Björk have the most spread out descriptive variables. Artist like the Beatles, Daft Punk, Fela Kuti and RHCP (Red Hot Chili Peppers) seem to have the most concise album audio features. Youssou N'Dour and Caetano Veloso seem to stay in the middle artists like Miles Davis & Björk and artists with concise album audio features.

### Detailed artist overview

```{r overview, echo=FALSE}
# Static plot code here
# Now, use 'track_info' in the 'text' aesthetic for tooltips
track_plot <- ggplot(corpus, aes(x = energy, y = valence, color = track.album.name, text = track_info, alpha = 0.8)) +
  geom_point() +
  facet_wrap(~artist) +
  labs(title = "Valence vs Energy colored by danceability per artist") + # Title for the color scale
  theme_minimal() +
  guides(alpha = FALSE) + # Optionally, remove the alpha guide if not needed
  theme(legend.text = element_text(size = 12),  # Increase legend text size
        legend.key.size = unit(1, "lines"),  # Increase legend key size
        legend.position = "none")  # Remove the legend

# Convert to plotly object with tooltips
track_plotly <- ggplotly(track_plot, tooltip = "text")

# Print the plotly plot
track_plotly



```

### testing
```{r}
bind_rows(
  lowest_track |> 
    compmus_self_similarity(pitches, "aitchison") |> 
    mutate(d = d / max(d), type = "Chroma"),
  lowest_track |> 
    compmus_self_similarity(timbre, "euclidean") |> 
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() + 
  labs(x = "", y = "")

# ## Highest
bind_rows(
  highest_track |>
    compmus_self_similarity(pitches, "aitchison") |>
    mutate(d = d / max(d), type = "Chroma"),
  highest_track |>
    compmus_self_similarity(timbre, "euclidean") |>
    mutate(d = d / max(d), type = "Timbre")
) |>
  mutate() |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_classic() +
  labs(x = "", y = "")
```





